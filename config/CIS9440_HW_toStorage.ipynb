{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdFah--xo4m7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bdFah--xo4m7",
    "outputId": "fe35d659-8905-45c1-e261-c725ef26d381"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: azure-storage-blob in c:\\users\\andrea\\anaconda3\\lib\\site-packages (12.19.1)\n",
      "Requirement already satisfied: azure-core<2.0.0,>=1.28.0 in c:\\users\\andrea\\anaconda3\\lib\\site-packages (from azure-storage-blob) (1.30.1)\n",
      "Requirement already satisfied: cryptography>=2.1.4 in c:\\users\\andrea\\anaconda3\\lib\\site-packages (from azure-storage-blob) (39.0.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\andrea\\anaconda3\\lib\\site-packages (from azure-storage-blob) (4.7.1)\n",
      "Requirement already satisfied: isodate>=0.6.1 in c:\\users\\andrea\\anaconda3\\lib\\site-packages (from azure-storage-blob) (0.6.1)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\andrea\\anaconda3\\lib\\site-packages (from azure-core<2.0.0,>=1.28.0->azure-storage-blob) (1.16.0)\n",
      "Requirement already satisfied: requests>=2.21.0 in c:\\users\\andrea\\anaconda3\\lib\\site-packages (from azure-core<2.0.0,>=1.28.0->azure-storage-blob) (2.31.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\andrea\\anaconda3\\lib\\site-packages (from cryptography>=2.1.4->azure-storage-blob) (1.15.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\andrea\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob) (2.21)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\andrea\\anaconda3\\lib\\site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.28.0->azure-storage-blob) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\andrea\\anaconda3\\lib\\site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.28.0->azure-storage-blob) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\andrea\\anaconda3\\lib\\site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.28.0->azure-storage-blob) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\andrea\\anaconda3\\lib\\site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.28.0->azure-storage-blob) (1.26.14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\andrea\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\andrea\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\andrea\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\andrea\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\andrea\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\andrea\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install azure-storage-blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f73fc0dc",
   "metadata": {
    "id": "f73fc0dc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "from azure.storage.blob import BlobServiceClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b47e317",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 869
    },
    "id": "4b47e317",
    "outputId": "7d2a5a82-cf81-4f49-a2de-800aca4ef076",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded data_chunk_offset_67000000.csv\n",
      "Successfully uploaded data_chunk_offset_72000000.csv\n",
      "Successfully uploaded data_chunk_offset_77000000.csv\n",
      "Successfully uploaded data_chunk_offset_82000000.csv\n",
      "Successfully uploaded data_chunk_offset_87000000.csv\n",
      "Successfully uploaded data_chunk_offset_92000000.csv\n",
      "Successfully uploaded data_chunk_offset_97000000.csv\n",
      "Successfully uploaded data_chunk_offset_102000000.csv\n",
      "Successfully uploaded data_chunk_offset_107000000.csv\n",
      "Successfully uploaded data_chunk_offset_111000000.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from io import StringIO\n",
    "\n",
    "\n",
    "# Your Azure Storage connection details\n",
    "config_file_path = \"config.json\"\n",
    "\n",
    "with open(config_file_path,\"r\") as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "connection_string = config[\"connectionString\"]\n",
    "container_name = \"violation\"\n",
    "\n",
    "# Initialize the Azure Blob service client\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "container_client = blob_service_client.get_container_client(container_name)\n",
    "\n",
    "# Base URL for your data source\n",
    "base_url = \"https://data.cityofnewyork.us/resource/nc67-uf89.csv\"\n",
    "\n",
    "# Parameters for API request and batch processing\n",
    "limit = 1000000\n",
    "total_records = 111625885\n",
    "offset = 63000000  # Start from the beginning if you aim to download the entire dataset\n",
    "\n",
    "# Dataframe to accumulate records\n",
    "accumulated_df = pd.DataFrame()\n",
    "\n",
    "while offset < total_records:\n",
    "    # Update the URL with the current offset\n",
    "    data_url = f\"{base_url}?$limit={limit}&$offset={offset}\"\n",
    "\n",
    "    try:\n",
    "        # Download the current chunk of data\n",
    "        df_chunk = pd.read_csv(data_url)\n",
    "\n",
    "        # Accumulate the data\n",
    "        accumulated_df = pd.concat([accumulated_df, df_chunk], ignore_index=True)\n",
    "\n",
    "        # Check if accumulated data reached the batch size of 5,000,000 records or if it's the last chunk\n",
    "        if len(accumulated_df) >= 5000000 or offset + limit >= total_records:\n",
    "            # Convert the accumulated DataFrame to a CSV string\n",
    "            csv_data = StringIO()\n",
    "            accumulated_df.to_csv(csv_data, index=False)\n",
    "            csv_data.seek(0)\n",
    "\n",
    "            # Define the blob name in your container\n",
    "            blob_name = f\"data_chunk_offset_{offset}.csv\"\n",
    "\n",
    "            # Upload the batch to Azure Blob Storage\n",
    "            blob_client = container_client.get_blob_client(blob_name)\n",
    "            blob_client.upload_blob(csv_data.getvalue(), overwrite=True)\n",
    "            print(f\"Successfully uploaded {blob_name}\")\n",
    "\n",
    "            # Reset the accumulated dataframe\n",
    "            accumulated_df = pd.DataFrame()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during data fetch/upload: {e}\")\n",
    "        break  # or handle retry logic\n",
    "\n",
    "    # Increment the offset for the next loop iteration\n",
    "    offset += limit\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
